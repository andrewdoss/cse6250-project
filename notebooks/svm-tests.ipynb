{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "sys.path.append('../')\n",
    "data_path = '../data/'\n",
    "from icd9 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'{data_path}/restricted_mimic_iii/labeled_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use basic text cleaning functions from reference book\n",
    "# Citation: Python Machine Learning 2nd Edition, Raschka\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all text\n",
    "data['text'] = data['text'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I will follow Perotte 2014 and use only the top 10,000 most frequent unigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, fit tfidf to training data and transform other splits\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                                   tokenizer=tokenize,\n",
    "                                   ngram_range=(1,1),\n",
    "                                   max_features=10000)\n",
    "\n",
    "train_data = data.loc[data['split']=='train',:].reset_index()\n",
    "val_data = data.loc[data['split']=='val',:].reset_index()\n",
    "\n",
    "train_X = tfidf_vectorizer.fit_transform(train_data['text'].values)\n",
    "val_X = tfidf_vectorizer.transform(val_data['text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple demo of fitting hierarchical SVMs and then making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ICD-9 tree and build index for train_data\n",
    "tree = ICD9Tree(f'{data_path}node_desc.csv', f'{data_path}node_parent.csv')\n",
    "tree.index_df(train_data, codes='fcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models and make predictions for entire ICD-9 tree\n",
    "test_node = tree.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22406 model fits attempted.\n"
     ]
    }
   ],
   "source": [
    "tree.fit_hsvm(train_X, test_node, max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tree.predict_hsvm(val_X, test_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average recall: 0.3667077935714987\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions to actual, only looking at recall for now\n",
    "val_data['preds'] = pd.Series(preds).apply(set)\n",
    "val_data['recall'] = 0\n",
    "\n",
    "val_data['fcode'].fillna('', inplace=True)\n",
    "val_data['fcode'] = val_data['fcode'].str.split(';').apply(set)\n",
    "\n",
    "for idx, row in val_data.iterrows():\n",
    "    #print(len(row.fcode.intersection(row.preds)))\n",
    "    val_data.loc[idx, 'recall'] = len(row.fcode.intersection(row.preds)) / len(row.fcode)\n",
    "\n",
    "\n",
    "print(f'Average recall: {val_data.recall.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average recall: 0.06534520358534794\n"
     ]
    }
   ],
   "source": [
    "# As a crude baseline, I will repeat using shuffled data which should be even worse than majority voting\n",
    "preds2 = tree.predict_hsvm(val_X[np.random.permutation(np.arange(val_X.shape[0])),:], test_node)\n",
    "\n",
    "# Compare predictions to actual, only looking at recall for now\n",
    "val_data['preds'] = pd.Series(preds2).apply(set)\n",
    "val_data['recall'] = 0\n",
    "\n",
    "for idx, row in val_data.iterrows():\n",
    "    #print(len(row.fcode.intersection(row.preds)))\n",
    "    val_data.loc[idx, 'recall'] = len(row.fcode.intersection(row.preds)) / len(row.fcode)\n",
    "\n",
    "\n",
    "print(f'Average recall: {val_data.recall.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it's lifting pretty well relative to random guessing. Next step will be to compare to a majority class (a more realistic baseline), look at other metrics like precision/F1, and also compare to a flat SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytt)",
   "language": "python",
   "name": "pytt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
