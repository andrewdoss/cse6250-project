{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "sys.path.append('../')\n",
    "data_path = '../data/'\n",
    "model_path = '../models/'\n",
    "from icd9 import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'{data_path}restricted_mimic_iii/labeled_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use basic text cleaning functions from reference book\n",
    "# Citation: Python Machine Learning 2nd Edition, Raschka\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all text\n",
    "data['text'] = data['text'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I will follow Perotte 2014 and use only the top 10,000 most frequent unigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, fit tfidf to training data and transform other splits\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                                   tokenizer=tokenize,\n",
    "                                   ngram_range=(1,1),\n",
    "                                   max_features=10000)\n",
    "\n",
    "train_data = data.loc[data['split']=='train',:].reset_index()\n",
    "val_data = data.loc[data['split']=='val',:].reset_index()\n",
    "test_data = data.loc[data['split']=='test',:].reset_index()\n",
    "\n",
    "train_X = tfidf_vectorizer.fit_transform(train_data['text'].values)\n",
    "val_X = tfidf_vectorizer.transform(val_data['text'].values)\n",
    "test_X = tfidf_vectorizer.transform(test_data['text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple demo of fitting hierarchical SVMs and then making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ICD-9 tree and build index for train_data\n",
    "tree = ICD9Tree(f'{data_path}node_desc.csv', f'{data_path}node_parent.csv')\n",
    "tree.index_df(train_data, codes='fcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models and make predictions for entire ICD-9 tree\n",
    "test_node = tree.root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrain and save the models, flip the comments in the two cells below. Otherwise, simply load the saved models from pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.fit_hmodel(train_X, test_node, max_depth=None)\n",
    "#tree.save_models(model_path+'hsvm_C1.obj', model_type='h')\n",
    "tree.load_models(model_path+'hsvm_C1.obj', model_type='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.fit_fmodel(train_X, test_node, max_depth=None)\n",
    "#tree.save_models(model_path+'fsvm_C1.obj', model_type='f')\n",
    "tree.load_models(model_path+'fsvm_C1.obj', model_type='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_preds_train = tree.predict_hmodel(train_X, test_node)\n",
    "h_preds_val = tree.predict_hmodel(val_X, test_node)\n",
    "h_preds_test = tree.predict_hmodel(test_X, test_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_preds_train = tree.predict_fmodel(train_X, test_node)\n",
    "f_preds_val = tree.predict_fmodel(val_X, test_node)\n",
    "f_preds_test = tree.predict_fmodel(test_X, test_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for comparing flat and hierarchical models\n",
    "def flat_v_hier_eval(df, h_preds, f_preds, codes='fcode'):\n",
    "    \"\"\"Helper for evaluating flat v. hierarchical models.\n",
    "    \n",
    "    df : pandas DataFrame\n",
    "        The evaluation set features and true labels.\n",
    "    h_preds : list of lists of code Strings\n",
    "        Labels predicted by the hierarchical model.\n",
    "    f_preds : list of lists of code Strings\n",
    "        Labels predicted by the flat model.\n",
    "    codes : String\n",
    "        The name of the column containing the codes as \";\" delimited\n",
    "        String.\n",
    "    \"\"\"\n",
    "    df['h_preds'] = pd.Series(h_preds).apply(set)\n",
    "    df['f_preds'] = pd.Series(f_preds).apply(set)\n",
    "\n",
    "    df['h_match'] = 0\n",
    "    df['f_match'] = 0\n",
    "\n",
    "    df[codes].fillna('', inplace=True)\n",
    "    df[codes] = df[codes].str.split(';').apply(set)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        df.loc[idx, 'h_match'] = len(row.fcode.intersection(row.h_preds))\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        df.loc[idx, 'f_match'] = len(row.fcode.intersection(row.f_preds)) \n",
    "        \n",
    "    def f1_score(prec, recall):\n",
    "        \"\"\"Compute f1 score\n",
    "        \n",
    "        Source: https://en.wikipedia.org/wiki/F1_score\n",
    "        \"\"\"\n",
    "        return 2 * (prec * recall)/(prec + recall)\n",
    "\n",
    "    h_recall = df['h_match'].sum() / df['fcode'].apply(len).sum()\n",
    "    h_prec = df['h_match'].sum() / df['h_preds'].apply(len).sum()\n",
    "\n",
    "    f_recall = df['f_match'].sum() / df['fcode'].apply(len).sum()\n",
    "    f_prec = df['f_match'].sum() / df['f_preds'].apply(len).sum()\n",
    "\n",
    "    print(f'Flat micro-avg metrics: precision = {f_prec:.2f}, recall = {f_recall:.2f}, f1 = {f1_score(f_prec, f_recall):.2f}')\n",
    "    print(f'Hierarchical micro-avg metrics: precision = {h_prec:.2f}, recall = {h_recall:.2f}, f1 = {f1_score(h_prec, h_recall):.2f}')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat micro-avg metrics: precision = 0.79, recall = 0.49, f1 = 0.6014615492289305\n",
      "Hierarchical micro-avg metrics: precision = 0.72, recall = 0.64, f1 = 0.6800467863386154\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models on training set\n",
    "flat_v_hier_eval(train_data, h_preds_train, f_preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat micro-avg metrics: precision = 0.62, recall = 0.23, f1 = 0.3400204185809086\n",
      "Hierarchical micro-avg metrics: precision = 0.50, recall = 0.34, f1 = 0.4062975072367188\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models on validation set\n",
    "flat_v_hier_eval(val_data, h_preds_val, f_preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat micro-avg metrics: precision = 0.61, recall = 0.23, f1 = 0.33221391687430035\n",
      "Hierarchical micro-avg metrics: precision = 0.49, recall = 0.33, f1 = 0.3938021874632483\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models on test set\n",
    "flat_v_hier_eval(test_data, h_preds_test, f_preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cannot compare directly to Perotte 2014, as so far I have only run this on MIMIC III using the CAML paper splits. However, these numbers are similar so far. Perotte found f1/precision/recall of 0.21/0.56/0.13 and 0.29/0.39/0.23 for the flat and hierarchical SVM models, respectively. \n",
    "\n",
    "In comparison, I am finding precision/recall of 0.33/0.61/0.23 and 0.39/0.49/0.33 for the flat and hierarchical SVM models, respectively.\n",
    "\n",
    "A MIMIC III comparison point is micro-f1 score with Mullenbach et. al (2018). Mullenbach et. al found 0.272 with a flat logistic regression model (very similar model to a linear SVC) while I found 0.332 with a flat linear SVC and 0.394 with a hierarchical SVC. \n",
    "\n",
    "Note: I am using the strict version of precision/recall where only exact code matches are considered. Perotte also presents a less strict version of precision/recall where ancestors/descendents are considered for a true positive match and descendents are considered for overriding an otherwise false negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytt)",
   "language": "python",
   "name": "pytt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
