{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC III clinical records prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from icd9 import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data path below points to a directory containing the \"DIAGNOSES_ICD\", \"NOTEEVENTS\", and \"PROCEDURES_ICD\" files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to standardize ICD-9 codes and normalize the results by hospital admission ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICD-9 Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['hadm_id', 'icd9_code']\n",
    "\n",
    "diag_icd = pd.read_csv(f'{data_path}/restricted_mimic_iii/DIAGNOSES_ICD.csv')\n",
    "diag_icd.columns = [name.lower() for name in diag_icd.columns]\n",
    "diag_icd = diag_icd.loc[:, keep_cols]\n",
    "diag_icd.dropna(subset=['icd9_code'], inplace=True)\n",
    "\n",
    "proc_icd = pd.read_csv(f'{data_path}/restricted_mimic_iii/PROCEDURES_ICD.csv')\n",
    "proc_icd.columns = [name.lower() for name in proc_icd.columns]\n",
    "proc_icd = proc_icd.loc[:, keep_cols]\n",
    "proc_icd.dropna(subset=['icd9_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIMIC III documentation provides the following clarification:\n",
    "\n",
    "\"The code field for the ICD-9-CM Principal and Other Diagnosis Codes is six characters in length, with the decimal point implied between the third and fourth digit for all diagnosis codes other than the V codes. The decimal is implied for V codes between the second and third digit.\"\n",
    "\n",
    "I will reformat the ICD-9 codes to a string format with decimal. The format needs to be consistent with the ICD-9 tree object naming convention.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be vectorized using Series op's but dataset is small\n",
    "# enough and apply method is more readable for this reformatting\n",
    "# Credit: https://github.com/jamesmullenbach/caml-mimic\n",
    "def format_codes(code, is_diag):\n",
    "    \"\"\"Reformat codes to match ICD-9 tree.\"\"\"\n",
    "    str_code = str(code)\n",
    "    if is_diag:\n",
    "        if str_code[0] == 'E':\n",
    "            if len(str_code) > 4:\n",
    "                str_code = str_code[:4] + '.' + str_code[4:]\n",
    "        else:\n",
    "            if len(str_code) > 3:\n",
    "                str_code = str_code[:3] + '.' + str_code[3:]\n",
    "    else:\n",
    "        if len(str_code) > 2:\n",
    "            str_code = str_code[:2] + '.' + str_code[2:]\n",
    "    return str_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat ICD-9 codes to match ICD-9 tree class\n",
    "diag_icd['fcode'] = diag_icd['icd9_code'].apply(format_codes, is_diag=True)\n",
    "proc_icd['fcode'] = proc_icd['icd9_code'].apply(format_codes, is_diag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will instantiate an ICD-9 tree object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ICD9Tree(f'{data_path}node_desc.csv', f'{data_path}node_parent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24% of events dropped.\n"
     ]
    }
   ],
   "source": [
    "# Join all ICD-9 codes together and check proportion that\n",
    "# do not match the tree\n",
    "all_icd = pd.concat([diag_icd, proc_icd], axis=0).drop('icd9_code', axis=1)\n",
    "match_icd = all_icd.loc[all_icd['fcode'].apply(lambda x: x in tree.nodes),:]\n",
    "perc_drop = 100*(1 - match_icd.shape[0] / all_icd.shape[0])\n",
    "print('{:.2f}% of events dropped.'.format(perc_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I checked some of the dropped codes and they do not appear to exist in the ICD-9 diagnosis or procedure hierarchies. This makes me think that they are erroneous. Rather than trying to infer the intended codes, I will simply drop the small number of affected events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I need to group the ICD-9 codes by hospital admission ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = match_icd.groupby('hadm_id')['fcode'] \\\n",
    "                     .apply(set).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(f'{data_path}/restricted_mimic_iii/NOTEEVENTS.csv', parse_dates=['CHARTDATE'],\n",
    "                       low_memory=False)\n",
    "notes_df.columns = [name.lower() for name in notes_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the category and description values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset down to discharge summaries only\n",
    "keep_cols = ['hadm_id', 'text']\n",
    "notes_df = notes_df.loc[notes_df['category'] == 'Discharge summary',\n",
    "                        keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate discharge notes by hadm_id\n",
    "notes_df = notes_df.groupby('hadm_id')['text'] \\\n",
    "                   .apply(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Notes to Labels and Assign Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = pd.read_csv(f'{data_path}train_full_hadm_ids.csv',\n",
    "                        header=None, names=['hadm_id'])\n",
    "val_ids = pd.read_csv(f'{data_path}dev_full_hadm_ids.csv',\n",
    "                      header=None, names=['hadm_id'])\n",
    "test_ids = pd.read_csv(f'{data_path}test_full_hadm_ids.csv',\n",
    "                       header=None, names=['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_50_ids = set(pd.read_csv(f'{data_path}train_50_hadm_ids.csv',\n",
    "                               header=None, names=['hadm_id'])['hadm_id'].values)\n",
    "val_50_ids = set(pd.read_csv(f'{data_path}dev_50_hadm_ids.csv',\n",
    "                             header=None, names=['hadm_id'])['hadm_id'].values)\n",
    "test_50_ids = set(pd.read_csv(f'{data_path}test_50_hadm_ids.csv',\n",
    "                              header=None, names=['hadm_id'])['hadm_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notes = notes_df.merge(train_ids, on='hadm_id', how='inner')\n",
    "train_notes['split'] = 'train'\n",
    "val_notes = notes_df.merge(val_ids, on='hadm_id', how='inner')\n",
    "val_notes['split'] = 'val'\n",
    "test_notes = notes_df.merge(test_ids, on='hadm_id', how='inner')\n",
    "test_notes['split'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notes = pd.concat([train_notes, val_notes, test_notes], axis=0) \\\n",
    "                  .merge(labels_df, on='hadm_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign top 50 splits from CAML paper/repo\n",
    "def top_50_splits(hadm_id):\n",
    "    if hadm_id in train_50_ids:\n",
    "        split='train'\n",
    "    elif hadm_id in val_50_ids:\n",
    "        split = 'val'\n",
    "    elif hadm_id in test_50_ids:\n",
    "        split = 'test'\n",
    "    else:\n",
    "        split = 'other'\n",
    "    return split\n",
    "\n",
    "labeled_notes['50_split'] = labeled_notes['hadm_id'].apply(top_50_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'995.92', '599.0', '511.9', '93.90', '38.93', '99.15', '585.9', '272.4', '96.71', '96.04', '285.1', 'V45.81', 'V15.82', '305.1', '584.9', '530.81', '287.5', '401.9', '518.81', '38.91', '427.31', 'V58.61', '486', '276.1', '88.72', '424.0', '410.71', '39.61', '96.72', '276.2', 'V45.82', '37.23', '88.56', '244.9', '250.00', '272.0', '507.0', '412', '285.9', '403.90', '428.0', '038.9', '311', '496', '39.95', '37.22', '414.01', '96.6', '99.04', '36.15'}\n"
     ]
    }
   ],
   "source": [
    "# Get top 50 labels\n",
    "def samples_per_label(labels):\n",
    "    \"\"\"Counts samples per label in labels\"\"\"\n",
    "    temp_counts = defaultdict(int)\n",
    "    for label_set in labels:\n",
    "        for label in label_set:\n",
    "            temp_counts[label] += 1\n",
    "    return temp_counts\n",
    "\n",
    "\n",
    "label_counts = samples_per_label(labeled_notes['fcode'])\n",
    "top_50 = set(pd.Series(label_counts).sort_values(ascending=False) \\\n",
    "                                    .iloc[:50] \\\n",
    "                                    .index \\\n",
    "                                    .values)\n",
    "print(top_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter top 50\n",
    "def filter_top_50(in_labels):\n",
    "    out_labels = set()\n",
    "    for label in in_labels:\n",
    "        if label in top_50:\n",
    "            out_labels.add(label)\n",
    "    return out_labels\n",
    "\n",
    "labeled_notes['fcode50'] = labeled_notes['fcode'].apply(filter_top_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notes['fcode'] = labeled_notes['fcode'].apply(lambda x: ';'.join(x))\n",
    "labeled_notes['fcode50'] = labeled_notes['fcode50'].apply(lambda x: ';'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_notes.to_csv(f'{data_path}/restricted_mimic_iii/labeled_notes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytt)",
   "language": "python",
   "name": "pytt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
